{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep-Qlearning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fd7b9d452c4441ff8e86719ce4584041": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_21865c5ef1bd4b36a08f0ad88fd831ac",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f53461fb057b4062910f89821dbbc0bc",
              "IPY_MODEL_616508684a3e4b698e0a4df4ac2e0c0a"
            ]
          }
        },
        "21865c5ef1bd4b36a08f0ad88fd831ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f53461fb057b4062910f89821dbbc0bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c057f84b3ecc4bcd8377287f73be2567",
            "_dom_classes": [],
            "description": " 36%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 2001,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 713,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_242467ffd4db47dea74d853bd520aff7"
          }
        },
        "616508684a3e4b698e0a4df4ac2e0c0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9b7857a2c5c14794a5dc852e8317681e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 713/2001 [07:41&lt;11:24,  1.88it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_367bbf3cea3f46dfbaebcc3f0a32d2c8"
          }
        },
        "c057f84b3ecc4bcd8377287f73be2567": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "242467ffd4db47dea74d853bd520aff7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9b7857a2c5c14794a5dc852e8317681e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "367bbf3cea3f46dfbaebcc3f0a32d2c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jizAanpo_j4O"
      },
      "source": [
        "! rm -rf Reinforcement_Learning_Othello"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udpHmn7pBbbo",
        "outputId": "5c754448-c78b-45de-e180-362d91e174ab"
      },
      "source": [
        "! git clone https://github.com/naomiehl/Reinforcement_Learning_Othello"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Reinforcement_Learning_Othello'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 19 (delta 7), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (19/19), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN2gIlIyBsVs"
      },
      "source": [
        "! cp Reinforcement_Learning_Othello/* ."
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o330eGd8BzMa"
      },
      "source": [
        "from environment import OthelloEnv\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "from collections import namedtuple\r\n",
        "from itertools import count\r\n",
        "import random"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2j6GXYfCDdD"
      },
      "source": [
        "env = OthelloEnv(n=8)\r\n",
        "env.reset()\r\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOHbp2VACM_z"
      },
      "source": [
        "class DQN(nn.Module):\r\n",
        "    def __init__(self, n=8, n_channels=3):\r\n",
        "        super(DQN, self).__init__()\r\n",
        "        self.n_channels = n_channels\r\n",
        "        self.convo = nn.Sequential(\r\n",
        "            nn.Conv2d(n_channels, n_channels*4, 3, stride=1, padding=1),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.BatchNorm2d(n_channels*4),\r\n",
        "            nn.Conv2d(n_channels*4, n_channels*8, 3, stride=1, padding=1),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.BatchNorm2d(n_channels*8),\r\n",
        "            nn.Conv2d(n_channels*8, n_channels*16, 3, stride=1, padding=1),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.BatchNorm2d(n_channels*16),\r\n",
        "            nn.Conv2d(n_channels*16, n_channels*32, 3, stride=1, padding=1),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.BatchNorm2d(n_channels*32)\r\n",
        "        )\r\n",
        "\r\n",
        "        self.fc1 = nn.Linear(n*n*n_channels*32, 1024)\r\n",
        "        self.bnfc = nn.BatchNorm1d(1024)\r\n",
        "        self.fc2 = nn.Linear(1024, n*n+1)  # including turn-skip\r\n",
        "\r\n",
        "    def forward(self, states):\r\n",
        "        states = nn.functional.one_hot(states + 1, num_classes=self.n_channels)\r\n",
        "        states = states.to(torch.float).transpose(2, -1).squeeze(1)\r\n",
        "\r\n",
        "        x = self.convo(states)\r\n",
        "\r\n",
        "        x = x.view(x.shape[0], -1)\r\n",
        "        x = self.fc1(x)\r\n",
        "        x = self.bnfc(F.relu(x))\r\n",
        "\r\n",
        "        return self.fc2(x)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fp3I-EFVZre"
      },
      "source": [
        "class DQNAgent:\r\n",
        "    def __init__(self, env, color, device=device, n_channels=3, lr=0.01):\r\n",
        "        self.q_model = DQN(env.n, n_channels).to(device)\r\n",
        "        self.target_model = DQN(env.n, n_channels).to(device)\r\n",
        "        self.update_target()\r\n",
        "        self.target_model.eval()\r\n",
        "        self.optimizer = torch.optim.RMSprop(self.q_model.parameters(), lr=lr)\r\n",
        "        self.buffer = ReplayBuffer(10000)\r\n",
        "        self.color = color\r\n",
        "\r\n",
        "    def draw_action(self, env, s, epsilon):\r\n",
        "        with torch.no_grad():\r\n",
        "            values = self.q_model(s).reshape(-1)\r\n",
        "        valid_moves = env.get_valid_moves(self.color)\r\n",
        "        if len(valid_moves) > 0:\r\n",
        "            if np.random.rand() <= 1 - epsilon:\r\n",
        "                valid_moves_ind = [env.coord2ind(p) for p in valid_moves]\r\n",
        "                action = valid_moves_ind[torch.argmax(values[valid_moves_ind])]\r\n",
        "                return env.ind2coord(action), values[action]\r\n",
        "            else:\r\n",
        "                action = valid_moves[np.random.randint(0, len(valid_moves))]\r\n",
        "                return action , values[env.coord2ind(action)]\r\n",
        "            \r\n",
        "        else:\r\n",
        "            return None, values[env.n * env.n]\r\n",
        "\r\n",
        "    def update_target(self):\r\n",
        "         self.target_model.load_state_dict(self.q_model.state_dict())\r\n",
        "\r\n",
        "class RandomAgent:\r\n",
        "    def __init__(self, color):\r\n",
        "        self.color = color\r\n",
        "    \r\n",
        "    def draw_action(self, env, s, epsilon):\r\n",
        "        valid_moves = env.get_valid_moves(self.color)\r\n",
        "        if len(valid_moves) > 0:\r\n",
        "            action = valid_moves[np.random.randint(0, len(valid_moves))]\r\n",
        "            return action, 1. / len(valid_moves)\r\n",
        "        else:\r\n",
        "            return None, 0\r\n",
        "\r\n",
        "\r\n",
        "class OthelloGame:\r\n",
        "    def __init__(self, agent_white, agent_black):\r\n",
        "        self.white = agent_white\r\n",
        "        self.black = agent_black\r\n",
        "\r\n",
        "    def get_agent(self, color):\r\n",
        "        if color == 1:\r\n",
        "            return self.white\r\n",
        "        else:\r\n",
        "            return self.black"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipbNdmfFyuqu"
      },
      "source": [
        "Transition = namedtuple('Transition',\r\n",
        "                        ('state', 'action', 'next_state', 'reward'))\r\n",
        "\r\n",
        "class ReplayBuffer(object):\r\n",
        "    def __init__(self, capacity):\r\n",
        "        self.capacity = capacity\r\n",
        "        self.memory = []\r\n",
        "        self.position = 0\r\n",
        "\r\n",
        "    def push(self, *args):\r\n",
        "        \"\"\"Saves a transition.\"\"\"\r\n",
        "        if len(self.memory) < self.capacity:\r\n",
        "            self.memory.append(None)\r\n",
        "        self.memory[self.position] = Transition(*args)\r\n",
        "        self.position = (self.position + 1) % self.capacity\r\n",
        "\r\n",
        "    def sample(self, batch_size):\r\n",
        "        return random.sample(self.memory, batch_size)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.memory)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Duw44fWmIsWm"
      },
      "source": [
        "def optimize_model(agent, batch_size, device=device, gamma=0.99):\r\n",
        "    agent.q_model.train()\r\n",
        "\r\n",
        "    if len(agent.buffer) < batch_size:\r\n",
        "        agent.q_model.eval()\r\n",
        "        return\r\n",
        "    transitions = agent.buffer.sample(batch_size)\r\n",
        "    batch = Transition(*zip(*transitions))\r\n",
        "\r\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\r\n",
        "                                            batch.next_state)), device=device, dtype=torch.bool)\r\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\r\n",
        "\r\n",
        "    reward_batch = torch.tensor(batch.reward, device=device)\r\n",
        "    state_batch = torch.cat(batch.state)\r\n",
        "    action_batch = torch.cat(batch.action)\r\n",
        "\r\n",
        "    state_action_values = agent.q_model(state_batch).gather(1, action_batch)\r\n",
        "\r\n",
        "    next_state_values = torch.zeros(batch_size, device=device)\r\n",
        "    next_state_values[non_final_mask] = agent.target_model(non_final_next_states).max(1)[0].detach()\r\n",
        "\r\n",
        "    expected_state_action_values = next_state_values * gamma + reward_batch\r\n",
        "\r\n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\r\n",
        "\r\n",
        "    agent.optimizer.zero_grad()\r\n",
        "    loss.backward()\r\n",
        "    for param in agent.q_model.parameters():\r\n",
        "        param.grad.data.clamp_(-1, 1)\r\n",
        "    agent.optimizer.step()\r\n",
        "    agent.q_model.eval()\r\n",
        "\r\n",
        "def state_numpy_to_tensor(state, device=device):\r\n",
        "    state = torch.from_numpy(state.astype(np.int64)).unsqueeze(0).unsqueeze(0)\r\n",
        "    return state.to(device)\r\n",
        "\r\n",
        "def train_one_episode(env, game, color, device=device, batch_size=128, gamma=0.99, epsilon=0.1):\r\n",
        "    game.get_agent(color).q_model.eval()\r\n",
        "\r\n",
        "    state = env.reset()\r\n",
        "    state = state_numpy_to_tensor(state)\r\n",
        "    done = False\r\n",
        "\r\n",
        "    while not done:\r\n",
        "        # Player plays\r\n",
        "        action, value = game.get_agent(env.turn).draw_action(env, state, epsilon)\r\n",
        "        if action is not None:\r\n",
        "            opp_state, reward, done, info = env.step(action)\r\n",
        "            action = torch.tensor([[env.coord2ind(action)]], device=device, dtype=torch.int64)\r\n",
        "            opp_state = state_numpy_to_tensor(opp_state)\r\n",
        "        else:\r\n",
        "            opp_state = state\r\n",
        "            env.turn *= -1\r\n",
        "            action = torch.tensor([[env.n*env.n]], device=device, dtype=torch.int64)\r\n",
        "\r\n",
        "        if env.turn == color:\r\n",
        "        # if next turn belongs to player color, then let him play, no need to update this agent\r\n",
        "            state = opp_state\r\n",
        "            continue\r\n",
        "        \r\n",
        "        # Opponent plays\r\n",
        "        opp_action, _ = game.get_agent(env.turn).draw_action(env, opp_state, epsilon)\r\n",
        "        if opp_action is not None and not done:\r\n",
        "            new_state, reward, done, info = env.step(opp_action)\r\n",
        "            new_state = state_numpy_to_tensor(new_state)\r\n",
        "\r\n",
        "            if reward * color > 0:\r\n",
        "                reward = 1.\r\n",
        "            elif reward *color < 0:\r\n",
        "                reward = -1.\r\n",
        "            elif done:\r\n",
        "                reward = 0.5\r\n",
        "\r\n",
        "            reward = torch.tensor([reward], device=device, dtype=torch.float)\r\n",
        "        elif done:\r\n",
        "            new_state = None\r\n",
        "        else:\r\n",
        "            new_state = opp_state\r\n",
        "            env.turn *= -1\r\n",
        "    \r\n",
        "        game.get_agent(color).buffer.push(state, action, new_state, reward)\r\n",
        "\r\n",
        "        optimize_model(game.get_agent(color), batch_size, device, gamma)\r\n",
        "\r\n",
        "        # Next turn\r\n",
        "        state = new_state\r\n",
        "\r\n",
        "def score_multi_episode(env, game, color, device=device, num_episodes=100):\r\n",
        "    '''Trained agent plays against an agent with random policy'''\r\n",
        "    game.get_agent(color).q_model.eval()\r\n",
        "    num_success = 0\r\n",
        "    num_cons_success = [0]\r\n",
        "    results = []\r\n",
        "    score = .0\r\n",
        "\r\n",
        "    for i in range(num_episodes):\r\n",
        "        state = env.reset()\r\n",
        "        state = state_numpy_to_tensor(state)\r\n",
        "        done = False\r\n",
        "\r\n",
        "        while not done:\r\n",
        "            if env.turn == color:\r\n",
        "                action, value = game.get_agent(color).draw_action(env, state, .0)\r\n",
        "            else:\r\n",
        "                valid_moves = env.get_valid_moves(env.turn)\r\n",
        "                if len(valid_moves) > 0:\r\n",
        "                    action = valid_moves[np.random.randint(0, len(valid_moves))]\r\n",
        "                else:\r\n",
        "                    action = None\r\n",
        "            if action is not None:\r\n",
        "                state, reward, done, info = env.step(action)\r\n",
        "                state = state_numpy_to_tensor(state)\r\n",
        "            else:\r\n",
        "                env.turn *= -1\r\n",
        "            \r\n",
        "        if env.score() * color >= 0:\r\n",
        "            num_success += 1\r\n",
        "            num_cons_success[-1] += 1\r\n",
        "            if env.score != 0:\r\n",
        "                score += 1.\r\n",
        "            else:\r\n",
        "                score += 0.5\r\n",
        "        else:\r\n",
        "            num_cons_success.append(0)\r\n",
        "            score -= 1.\r\n",
        "\r\n",
        "        results.append(reward)\r\n",
        "    return num_success, max(num_cons_success), score, results"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExtJv7uBVHgO"
      },
      "source": [
        "np.random.seed(0)\r\n",
        "torch.manual_seed(0)\r\n",
        "random.seed(0)\r\n",
        "\r\n",
        "game = OthelloGame(DQNAgent(env, 1, lr=0.01), RandomAgent(-1))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501,
          "referenced_widgets": [
            "fd7b9d452c4441ff8e86719ce4584041",
            "21865c5ef1bd4b36a08f0ad88fd831ac",
            "f53461fb057b4062910f89821dbbc0bc",
            "616508684a3e4b698e0a4df4ac2e0c0a",
            "c057f84b3ecc4bcd8377287f73be2567",
            "242467ffd4db47dea74d853bd520aff7",
            "9b7857a2c5c14794a5dc852e8317681e",
            "367bbf3cea3f46dfbaebcc3f0a32d2c8"
          ]
        },
        "id": "WgGNgAipmgwG",
        "outputId": "121f6fb8-591a-4421-c5d3-b346e8d8110a"
      },
      "source": [
        "color = 1\r\n",
        "nb_episodes_per_agent = 10\r\n",
        "target_update = 10\r\n",
        "print_step = 200\r\n",
        "eps_start = 0.9\r\n",
        "eps_end = 0.05\r\n",
        "eps_decay = 200\r\n",
        "\r\n",
        "for i in tqdm(range(2001)):\r\n",
        "    train_one_episode(env, game, color, epsilon=eps_end+(eps_start-eps_end)*np.exp(-i))\r\n",
        "\r\n",
        "    if i % target_update == 0:\r\n",
        "        game.get_agent(color).update_target()\r\n",
        "\r\n",
        "    # if i % nb_episodes_per_agent == 0:\r\n",
        "    #     color *= -1\r\n",
        "\r\n",
        "    if i % print_step == 0:\r\n",
        "        num_success, max_cons_success, score, _ = score_multi_episode(env, game, 1)\r\n",
        "        print(\"White ... Episode: {}, Number of wins: {}, Max number of consecutive wins: {}, Total score: {:.1f}\".format(i, num_success, max_cons_success, score))\r\n",
        "        # num_success, max_cons_success, score, _ = score_multi_episode(env, game, -1)\r\n",
        "        # print(\"Black ... Episode: {}, Number of wins: {}, Max number of consecutive wins: {}, Total score: {:.1f}\".format(i, num_success, max_cons_success, score))\r\n",
        "        eval_loss = .0"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd7b9d452c4441ff8e86719ce4584041",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2001.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "White ... Episode: 0, Number of wins: 57, Max number of consecutive wins: 7, Total score: 14.0\n",
            "White ... Episode: 200, Number of wins: 34, Max number of consecutive wins: 4, Total score: -32.0\n",
            "White ... Episode: 400, Number of wins: 32, Max number of consecutive wins: 3, Total score: -36.0\n",
            "White ... Episode: 600, Number of wins: 36, Max number of consecutive wins: 7, Total score: -28.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-0d8945f6995d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_one_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtarget_update\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-8b7be3b9a842>\u001b[0m in \u001b[0;36mtrain_one_episode\u001b[0;34m(env, game, color, device, batch_size, gamma, epsilon)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# Next turn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-8b7be3b9a842>\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m(agent, batch_size, device, gamma)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/rmsprop.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msquare_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                     \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msquare_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}