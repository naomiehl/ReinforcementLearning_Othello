{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep-Qlearning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e7740f37ddc846b0b1f4b3f6fdac8d67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cbb0168ccc45449dab77903ff2164a51",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_94cb47b5e0d84a5085c148ac06b2c509",
              "IPY_MODEL_abae601685b74a3899105cb7df2d2475"
            ]
          }
        },
        "cbb0168ccc45449dab77903ff2164a51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "94cb47b5e0d84a5085c148ac06b2c509": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9789e2f0158e48e8815dd253558434b3",
            "_dom_classes": [],
            "description": "  4%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 200001,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 7186,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7815e1513049463da0038b880d230ef7"
          }
        },
        "abae601685b74a3899105cb7df2d2475": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_adb8360218614228989256252fbdcedf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 7186/200001 [1:14:34&lt;27:20:34,  1.96it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_52a157512c2c491aa823ca94d4741662"
          }
        },
        "9789e2f0158e48e8815dd253558434b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7815e1513049463da0038b880d230ef7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "adb8360218614228989256252fbdcedf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "52a157512c2c491aa823ca94d4741662": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jizAanpo_j4O"
      },
      "source": [
        "! rm -rf Reinforcement_Learning_Othello"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udpHmn7pBbbo",
        "outputId": "1832d493-1ed9-49f8-9e99-882f615f69ef"
      },
      "source": [
        "! git clone https://github.com/naomiehl/Reinforcement_Learning_Othello"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Reinforcement_Learning_Othello'...\n",
            "remote: Enumerating objects: 22, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 22 (delta 8), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (22/22), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN2gIlIyBsVs"
      },
      "source": [
        "! cp Reinforcement_Learning_Othello/* ."
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o330eGd8BzMa"
      },
      "source": [
        "from environment import OthelloEnv\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "from collections import namedtuple\r\n",
        "from itertools import count\r\n",
        "import random"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2j6GXYfCDdD"
      },
      "source": [
        "env = OthelloEnv(n=8)\r\n",
        "env.reset()\r\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "EPS_START = 0.9\r\n",
        "EPS_END = 0.0\r\n",
        "EPS_DECAY = 400\r\n",
        "BATCH_SIZE = 128\r\n",
        "NUM_EPISODES_EVAL = 100\r\n",
        "GAMMA = 0.99\r\n",
        "LR = 0.05\r\n",
        "N_CHANNELS = 3"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOHbp2VACM_z"
      },
      "source": [
        "class DQN(nn.Module):\r\n",
        "    def __init__(self, n=8, n_channels=N_CHANNELS):\r\n",
        "        super(DQN, self).__init__()\r\n",
        "        self.n_channels = n_channels\r\n",
        "        self.convo = nn.Sequential(\r\n",
        "            nn.Conv2d(n_channels, 4, 3, stride=1, padding=1),\r\n",
        "            nn.LeakyReLU(),\r\n",
        "            nn.BatchNorm2d(4),\r\n",
        "            nn.Conv2d(4, 8, 3, stride=1, padding=1),\r\n",
        "            nn.LeakyReLU(),\r\n",
        "            nn.BatchNorm2d(8),\r\n",
        "            nn.Conv2d(8, 16, 3, stride=1, padding=1),\r\n",
        "            nn.LeakyReLU(),\r\n",
        "            nn.BatchNorm2d(16),\r\n",
        "        )\r\n",
        "\r\n",
        "        self.head = nn.Sequential(\r\n",
        "            nn.Linear(n*n*16, n*n),\r\n",
        "            nn.LeakyReLU(),\r\n",
        "            nn.BatchNorm1d(n*n),\r\n",
        "            nn.Linear(n*n, n*n+1),\r\n",
        "            nn.Hardtanh()\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, states):\r\n",
        "        states = nn.functional.one_hot(states + 1, num_classes=self.n_channels)\r\n",
        "        states = states.to(torch.float).transpose(2, -1).squeeze(1)\r\n",
        "\r\n",
        "        x = self.convo(states)\r\n",
        "        # x = states.to(torch.float)\r\n",
        "\r\n",
        "        x = x.view(x.shape[0], -1)\r\n",
        "        x = self.head(x)\r\n",
        "\r\n",
        "        return x"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fp3I-EFVZre"
      },
      "source": [
        "class DQNAgent:\r\n",
        "    def __init__(self, env, color, device=device, n_channels=3, lr=LR):\r\n",
        "        self.q_model = DQN(env.n, n_channels).to(device)\r\n",
        "        self.target_model = DQN(env.n, n_channels).to(device)\r\n",
        "        self.update_target_model()\r\n",
        "        self.target_model.eval()\r\n",
        "        self.optimizer = torch.optim.Adam(self.q_model.parameters(), lr=lr)\r\n",
        "        self.buffer = ReplayBuffer(10000)\r\n",
        "        self.color = color\r\n",
        "        self.steps_done = 0\r\n",
        "\r\n",
        "    def draw_action(self, env, s, epsilon=None):\r\n",
        "        self.steps_done += 1\r\n",
        "        if epsilon is None:\r\n",
        "            epsilon =  EPS_END + (EPS_START - EPS_END) * np.exp(-1. * self.steps_done / EPS_DECAY)\r\n",
        "\r\n",
        "        s *= self.color\r\n",
        "\r\n",
        "        with torch.no_grad():\r\n",
        "            values = self.q_model(s).reshape(-1)\r\n",
        "        valid_moves = env.get_valid_moves(self.color)\r\n",
        "        if len(valid_moves) > 0:\r\n",
        "            if np.random.rand() <= 1 - epsilon:\r\n",
        "                valid_moves_ind = [env.coord2ind(p) for p in valid_moves]\r\n",
        "                action = valid_moves_ind[torch.argmax(values[valid_moves_ind])]\r\n",
        "                return env.ind2coord(action), values[action]\r\n",
        "            else:\r\n",
        "                action = valid_moves[np.random.randint(0, len(valid_moves))]\r\n",
        "                return action , values[env.coord2ind(action)]\r\n",
        "            \r\n",
        "        else:\r\n",
        "            return None, values[env.n * env.n]\r\n",
        "\r\n",
        "    def update_target_model(self):\r\n",
        "         self.target_model.load_state_dict(self.q_model.state_dict())\r\n",
        "\r\n",
        "class RandomAgent:\r\n",
        "    def __init__(self, color):\r\n",
        "        self.color = color\r\n",
        "    \r\n",
        "    def draw_action(self, env, s, epsilon):\r\n",
        "        valid_moves = env.get_valid_moves(self.color)\r\n",
        "        if len(valid_moves) > 0:\r\n",
        "            action = valid_moves[np.random.randint(0, len(valid_moves))]\r\n",
        "            return action, 1. / len(valid_moves)\r\n",
        "        else:\r\n",
        "            return None, 0\r\n",
        "\r\n",
        "\r\n",
        "class OthelloGame:\r\n",
        "    def __init__(self, agent_white, agent_black):\r\n",
        "        self.white = agent_white\r\n",
        "        self.black = agent_black\r\n",
        "\r\n",
        "    def get_agent(self, color):\r\n",
        "        if color == 1:\r\n",
        "            return self.white\r\n",
        "        else:\r\n",
        "            return self.black\r\n",
        "    \r\n",
        "    def sync(self, color_optimized, color_update):\r\n",
        "        \"\"\"Copy model state of agent color_optimized to agent color_update\"\"\"\r\n",
        "        self.get_agent(color_update).q_model.load_state_dict(self.get_agent(color_optimized).q_model.state_dict())"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipbNdmfFyuqu"
      },
      "source": [
        "Transition = namedtuple('Transition',\r\n",
        "                        ('state', 'action', 'next_state', 'reward'))\r\n",
        "\r\n",
        "class ReplayBuffer(object):\r\n",
        "    def __init__(self, capacity):\r\n",
        "        self.capacity = capacity\r\n",
        "        self.memory = []\r\n",
        "        self.position = 0\r\n",
        "\r\n",
        "    def push(self, *args):\r\n",
        "        \"\"\"Saves a transition.\"\"\"\r\n",
        "        if len(self.memory) < self.capacity:\r\n",
        "            self.memory.append(None)\r\n",
        "        self.memory[self.position] = Transition(*args)\r\n",
        "        self.position = (self.position + 1) % self.capacity\r\n",
        "\r\n",
        "    def sample(self, batch_size):\r\n",
        "        return random.sample(self.memory, batch_size)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.memory)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Duw44fWmIsWm"
      },
      "source": [
        "def optimize_model(agent, batch_size=BATCH_SIZE, device=device, gamma=GAMMA):\r\n",
        "    agent.q_model.train()\r\n",
        "\r\n",
        "    if len(agent.buffer) < batch_size:\r\n",
        "        agent.q_model.eval()\r\n",
        "        return\r\n",
        "\r\n",
        "    transitions = agent.buffer.sample(batch_size)\r\n",
        "    batch = Transition(*zip(*transitions))\r\n",
        "\r\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\r\n",
        "                                            batch.next_state)), device=device, dtype=torch.bool)\r\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\r\n",
        "\r\n",
        "    reward_batch = torch.tensor(batch.reward, device=device)\r\n",
        "    state_batch = torch.cat(batch.state)\r\n",
        "    action_batch = torch.cat(batch.action)\r\n",
        "\r\n",
        "    state_action_values = agent.q_model(state_batch).gather(1, action_batch)\r\n",
        "\r\n",
        "    next_state_values = torch.zeros(batch_size, device=device)\r\n",
        "    next_state_values[non_final_mask] = agent.target_model(non_final_next_states).max(1)[0].detach()\r\n",
        "\r\n",
        "    expected_state_action_values = next_state_values * gamma + reward_batch\r\n",
        "\r\n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\r\n",
        "\r\n",
        "    agent.optimizer.zero_grad()\r\n",
        "    loss.backward()\r\n",
        "    # for param in agent.q_model.parameters():\r\n",
        "    #     param.grad.data.clamp_(-1, 1)\r\n",
        "    agent.optimizer.step()\r\n",
        "    agent.q_model.eval()\r\n",
        "\r\n",
        "def state_numpy_to_tensor(state, device=device):\r\n",
        "    state = torch.from_numpy(state.astype(np.int64)).unsqueeze(0).unsqueeze(0)\r\n",
        "    return state.to(device)\r\n",
        "\r\n",
        "def train_one_episode(env, game, color, device=device, batch_size=BATCH_SIZE, gamma=GAMMA, epsilon=None):\r\n",
        "    game.get_agent(color).q_model.eval()\r\n",
        "    game.get_agent(-color).q_model.eval()\r\n",
        "\r\n",
        "    state = env.reset()\r\n",
        "    state = state_numpy_to_tensor(state)\r\n",
        "    done = False\r\n",
        "\r\n",
        "    while not done:\r\n",
        "        # Player plays\r\n",
        "        action, value = game.get_agent(env.turn).draw_action(env, state, epsilon)\r\n",
        "        if action is not None:\r\n",
        "            opp_state, reward, done, info = env.step(action)\r\n",
        "            action = torch.tensor([[env.coord2ind(action)]], device=device, dtype=torch.int64)\r\n",
        "            opp_state = state_numpy_to_tensor(opp_state)\r\n",
        "        else:\r\n",
        "            opp_state = state\r\n",
        "            env.turn *= -1\r\n",
        "            action = torch.tensor([[env.n*env.n]], device=device, dtype=torch.int64)\r\n",
        "\r\n",
        "        if env.turn == color:\r\n",
        "        # if next turn belongs to player color, then let him play, no need to update this agent\r\n",
        "            state = opp_state\r\n",
        "            continue\r\n",
        "        \r\n",
        "        # Opponent plays\r\n",
        "        opp_action, _ = game.get_agent(env.turn).draw_action(env, opp_state, epsilon)\r\n",
        "        if opp_action is not None and not done:\r\n",
        "            new_state, reward, done, info = env.step(opp_action)\r\n",
        "            new_state = state_numpy_to_tensor(new_state)\r\n",
        "\r\n",
        "            if reward * color > 0:\r\n",
        "                reward = 1.\r\n",
        "            elif reward *color < 0:\r\n",
        "                reward = -1.\r\n",
        "\r\n",
        "            reward = torch.tensor([reward], device=device, dtype=torch.float)\r\n",
        "        elif done:\r\n",
        "            new_state = None\r\n",
        "        else:\r\n",
        "            new_state = opp_state\r\n",
        "            env.turn *= -1\r\n",
        "    \r\n",
        "        game.get_agent(color).buffer.push(state * color, action, new_state * color if new_state is not None else None, reward)\r\n",
        "\r\n",
        "        optimize_model(game.get_agent(color), batch_size, device, gamma)\r\n",
        "\r\n",
        "        # Next turn\r\n",
        "        state = new_state\r\n",
        "\r\n",
        "def score_multi_episode(env, game, color, device=device, num_episodes=NUM_EPISODES, epsilon = .0):\r\n",
        "    '''Trained agent plays against an agent with random policy'''\r\n",
        "    game.get_agent(color).q_model.eval()\r\n",
        "    num_success = 0\r\n",
        "    num_cons_success = [0]\r\n",
        "    results = []\r\n",
        "    score = .0\r\n",
        "\r\n",
        "    for i in range(num_episodes):\r\n",
        "        state = env.reset()\r\n",
        "        state = state_numpy_to_tensor(state)\r\n",
        "        done = False\r\n",
        "\r\n",
        "        while not done:\r\n",
        "            if env.turn == color:\r\n",
        "                action, value = game.get_agent(color).draw_action(env, state, epsilon)\r\n",
        "            else:\r\n",
        "                valid_moves = env.get_valid_moves(env.turn)\r\n",
        "                if len(valid_moves) > 0:\r\n",
        "                    action = valid_moves[np.random.randint(0, len(valid_moves))]\r\n",
        "                else:\r\n",
        "                    action = None\r\n",
        "            if action is not None:\r\n",
        "                state, reward, done, info = env.step(action)\r\n",
        "                state = state_numpy_to_tensor(state)\r\n",
        "            else:\r\n",
        "                env.turn *= -1\r\n",
        "            \r\n",
        "        if env.score() * color > 0:\r\n",
        "            num_success += 1\r\n",
        "            num_cons_success[-1] += 1\r\n",
        "            score += 1.\r\n",
        "        else:\r\n",
        "            num_cons_success.append(0)\r\n",
        "            if env.score() != 0:\r\n",
        "                score -= 1.\r\n",
        "\r\n",
        "        results.append(reward)\r\n",
        "    return num_success, max(num_cons_success), score, results"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExtJv7uBVHgO"
      },
      "source": [
        "np.random.seed(0)\r\n",
        "torch.manual_seed(0)\r\n",
        "random.seed(0)\r\n",
        "\r\n",
        "game = OthelloGame(DQNAgent(env, 1, lr=LR), DQNAgent(env, -1, lr=LR))\r\n",
        "game.sync(1, -1)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595,
          "referenced_widgets": [
            "e7740f37ddc846b0b1f4b3f6fdac8d67",
            "cbb0168ccc45449dab77903ff2164a51",
            "94cb47b5e0d84a5085c148ac06b2c509",
            "abae601685b74a3899105cb7df2d2475",
            "9789e2f0158e48e8815dd253558434b3",
            "7815e1513049463da0038b880d230ef7",
            "adb8360218614228989256252fbdcedf",
            "52a157512c2c491aa823ca94d4741662"
          ]
        },
        "id": "WgGNgAipmgwG",
        "outputId": "93cfd371-4792-457d-e568-6e6ec836e663"
      },
      "source": [
        "color = 1\r\n",
        "nb_episodes_per_agent = 1\r\n",
        "target_update = 10\r\n",
        "print_step = 500\r\n",
        "\r\n",
        "for i in tqdm(range(200001)):\r\n",
        "    train_one_episode(env, game, color)\r\n",
        "\r\n",
        "    # if i % 100 == 0:\r\n",
        "    #     print(env.render())\r\n",
        "    #     print(env.score())\r\n",
        "\r\n",
        "    if i % nb_episodes_per_agent == 0:\r\n",
        "        game.sync(color, -color)  # Update model for the other player\r\n",
        "        color *= -1\r\n",
        "    \r\n",
        "    if i % target_update == 0:\r\n",
        "        game.get_agent(color).update_target_model()\r\n",
        "        game.get_agent(-color).update_target_model()\r\n",
        "\r\n",
        "    if i % print_step == 0:\r\n",
        "        num_success, max_cons_success, score, _ = score_multi_episode(env, game, 1)\r\n",
        "        print(\"White ... Episode: {}, Number of wins: {}, Max number of consecutive wins: {}, Total score: {:.1f}\".format(i, num_success, max_cons_success, score))\r\n",
        "        num_success, max_cons_success, score, _ = score_multi_episode(env, game, -1)\r\n",
        "        print(\"Black ... Episode: {}, Number of wins: {}, Max number of consecutive wins: {}, Total score: {:.1f}\".format(i, num_success, max_cons_success, score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7740f37ddc846b0b1f4b3f6fdac8d67",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=200001.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "White ... Episode: 0, Number of wins: 74, Max number of consecutive wins: 12, Total score: 48.0\n",
            "Black ... Episode: 0, Number of wins: 54, Max number of consecutive wins: 6, Total score: 12.0\n",
            "White ... Episode: 500, Number of wins: 64, Max number of consecutive wins: 5, Total score: 29.0\n",
            "Black ... Episode: 500, Number of wins: 39, Max number of consecutive wins: 5, Total score: -17.0\n",
            "White ... Episode: 1000, Number of wins: 62, Max number of consecutive wins: 6, Total score: 29.0\n",
            "Black ... Episode: 1000, Number of wins: 48, Max number of consecutive wins: 7, Total score: 1.0\n",
            "White ... Episode: 1500, Number of wins: 65, Max number of consecutive wins: 13, Total score: 31.0\n",
            "Black ... Episode: 1500, Number of wins: 63, Max number of consecutive wins: 11, Total score: 29.0\n",
            "White ... Episode: 2000, Number of wins: 57, Max number of consecutive wins: 7, Total score: 17.0\n",
            "Black ... Episode: 2000, Number of wins: 53, Max number of consecutive wins: 6, Total score: 8.0\n",
            "White ... Episode: 2500, Number of wins: 57, Max number of consecutive wins: 9, Total score: 20.0\n",
            "Black ... Episode: 2500, Number of wins: 42, Max number of consecutive wins: 3, Total score: -13.0\n",
            "White ... Episode: 3000, Number of wins: 57, Max number of consecutive wins: 6, Total score: 16.0\n",
            "Black ... Episode: 3000, Number of wins: 45, Max number of consecutive wins: 7, Total score: -6.0\n",
            "White ... Episode: 3500, Number of wins: 57, Max number of consecutive wins: 7, Total score: 17.0\n",
            "Black ... Episode: 3500, Number of wins: 62, Max number of consecutive wins: 13, Total score: 30.0\n",
            "White ... Episode: 4000, Number of wins: 72, Max number of consecutive wins: 14, Total score: 45.0\n",
            "Black ... Episode: 4000, Number of wins: 50, Max number of consecutive wins: 6, Total score: 3.0\n",
            "White ... Episode: 4500, Number of wins: 52, Max number of consecutive wins: 5, Total score: 8.0\n",
            "Black ... Episode: 4500, Number of wins: 58, Max number of consecutive wins: 9, Total score: 19.0\n",
            "White ... Episode: 5000, Number of wins: 59, Max number of consecutive wins: 7, Total score: 21.0\n",
            "Black ... Episode: 5000, Number of wins: 58, Max number of consecutive wins: 8, Total score: 23.0\n",
            "White ... Episode: 5500, Number of wins: 56, Max number of consecutive wins: 11, Total score: 18.0\n",
            "Black ... Episode: 5500, Number of wins: 46, Max number of consecutive wins: 5, Total score: -3.0\n",
            "White ... Episode: 6000, Number of wins: 58, Max number of consecutive wins: 7, Total score: 21.0\n",
            "Black ... Episode: 6000, Number of wins: 65, Max number of consecutive wins: 12, Total score: 33.0\n",
            "White ... Episode: 6500, Number of wins: 63, Max number of consecutive wins: 6, Total score: 30.0\n",
            "Black ... Episode: 6500, Number of wins: 47, Max number of consecutive wins: 3, Total score: 5.0\n",
            "White ... Episode: 7000, Number of wins: 62, Max number of consecutive wins: 6, Total score: 28.0\n",
            "Black ... Episode: 7000, Number of wins: 62, Max number of consecutive wins: 7, Total score: 24.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSYgPMOMLfnC"
      },
      "source": [
        "print(env.render())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KglhpSqgLg9Z"
      },
      "source": [
        "from ipywidgets import widgets\r\n",
        "from IPython.display import display\r\n",
        "text = widgets.Text()\r\n",
        "display(text)\r\n",
        "state = env.reset()\r\n",
        "print(env.render())\r\n",
        "print(env.get_valid_moves(1))\r\n",
        "print(env.score())\r\n",
        "state = state_numpy_to_tensor(state)\r\n",
        "game.get_agent(-1).q_model.eval()\r\n",
        "action, value = game.get_agent(-1).draw_action(env, state, epsilon=.0)\r\n",
        "state, reward, done, info = env.step(action)\r\n",
        "print(env.render())\r\n",
        "print(env.get_valid_moves(1))\r\n",
        "print(reward)\r\n",
        "print(value)\r\n",
        "\r\n",
        "def handle_submit(sender):\r\n",
        "    human_step(env, eval(text.value))\r\n",
        "\r\n",
        "def human_step(env, action):\r\n",
        "    if action is not None:\r\n",
        "        state, _, done, info = env.step(action)\r\n",
        "    else:\r\n",
        "        pass\r\n",
        "    state = state_numpy_to_tensor(state)\r\n",
        "    action, _ = game.get_agent(-1).draw_action(env, state, epsilon=.0)\r\n",
        "    state, reward, done, info = env.step(action)\r\n",
        "    print(env.render())\r\n",
        "    print(env.get_valid_moves(1))\r\n",
        "    print(reward)\r\n",
        "\r\n",
        "text.on_submit(handle_submit)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjag8uYuRAmF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}