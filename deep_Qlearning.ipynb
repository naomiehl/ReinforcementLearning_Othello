{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep-Qlearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOHoqZJAxM+CdlsJnz16/6X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naomiehl/Reinforcement_Learning_Othello/blob/main/deep_Qlearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o330eGd8BzMa"
      },
      "source": [
        "from environment import OthelloEnv\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from tqdm import tqdm\r\n",
        "from collections import namedtuple\r\n",
        "from itertools import count\r\n",
        "import random\r\n",
        "from copy import deepcopy"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2j6GXYfCDdD"
      },
      "source": [
        "env = OthelloEnv(n=8)\r\n",
        "env.reset()\r\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "EPS_START = 0.9\r\n",
        "EPS_END = 0.0\r\n",
        "EPS_DECAY = 400\r\n",
        "BATCH_SIZE = 128\r\n",
        "NUM_EPISODES_EVAL = 100\r\n",
        "GAMMA = 0.99\r\n",
        "LR = 0.1\r\n",
        "N_CHANNELS = 3\r\n",
        "MINIMAX_DEPTH = 3"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOHbp2VACM_z"
      },
      "source": [
        "class DQN(nn.Module):\r\n",
        "    def __init__(self, n=8, n_channels=N_CHANNELS):\r\n",
        "        super(DQN, self).__init__()\r\n",
        "        self.n_channels = n_channels\r\n",
        "        self.convo = nn.Sequential(\r\n",
        "            nn.Conv2d(n_channels, 4, 3, stride=1, padding=1),\r\n",
        "            nn.LeakyReLU(),\r\n",
        "            nn.BatchNorm2d(4),\r\n",
        "            nn.Conv2d(4, 8, 3, stride=1, padding=1),\r\n",
        "            nn.LeakyReLU(),\r\n",
        "            nn.BatchNorm2d(8),\r\n",
        "            nn.Conv2d(8, 16, 3, stride=1, padding=1),\r\n",
        "            nn.LeakyReLU(),\r\n",
        "            nn.BatchNorm2d(16),\r\n",
        "        )\r\n",
        "\r\n",
        "        self.head = nn.Sequential(\r\n",
        "            nn.Linear(n*n*16, n*n),\r\n",
        "            nn.LeakyReLU(),\r\n",
        "            nn.BatchNorm1d(n*n),\r\n",
        "            nn.Linear(n*n, n*n),\r\n",
        "            nn.Hardtanh()\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, states):\r\n",
        "        states = nn.functional.one_hot(states + 1, num_classes=self.n_channels)\r\n",
        "        states = states.to(torch.float).transpose(2, -1).squeeze(1)\r\n",
        "\r\n",
        "        x = self.convo(states)\r\n",
        "        # x = states.to(torch.float)\r\n",
        "\r\n",
        "        x = x.view(x.shape[0], -1)\r\n",
        "        x = self.head(x)\r\n",
        "\r\n",
        "        return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fp3I-EFVZre"
      },
      "source": [
        "class DQNAgent:\r\n",
        "    def __init__(self, env, color, device=device, n_channels=3, lr=LR):\r\n",
        "        self.q_model = DQN(env.n, n_channels).to(device)\r\n",
        "        self.target_model = DQN(env.n, n_channels).to(device)\r\n",
        "        self.update_target_model()\r\n",
        "        self.target_model.eval()\r\n",
        "        self.optimizer = torch.optim.Adam(self.q_model.parameters(), lr=lr)\r\n",
        "        self.buffer = ReplayBuffer(10000)\r\n",
        "        self.color = color\r\n",
        "        self.steps_done = 0\r\n",
        "\r\n",
        "    def draw_action(self, env, s, epsilon=None):\r\n",
        "        self.steps_done += 1\r\n",
        "        if epsilon is None:\r\n",
        "            epsilon =  EPS_END + (EPS_START - EPS_END) * np.exp(-1. * self.steps_done / EPS_DECAY)\r\n",
        "\r\n",
        "        s *= self.color\r\n",
        "\r\n",
        "        with torch.no_grad():\r\n",
        "            values = self.q_model(s).reshape(-1)\r\n",
        "        valid_moves = env.get_valid_moves(self.color)\r\n",
        "        if len(valid_moves) > 0:\r\n",
        "            if np.random.rand() <= 1 - epsilon:\r\n",
        "                valid_moves_ind = [env.coord2ind(p) for p in valid_moves]\r\n",
        "                action = valid_moves[torch.argmax(values[valid_moves_ind])]\r\n",
        "                return action, values[env.coord2ind(action)]\r\n",
        "            else:\r\n",
        "                action = valid_moves[np.random.randint(0, len(valid_moves))]\r\n",
        "                return action , values[env.coord2ind(action)]\r\n",
        "            \r\n",
        "        else:\r\n",
        "            print(env.render())\r\n",
        "            return None, 0\r\n",
        "\r\n",
        "    def draw_action_minimax(self, env, s, depth=MINIMAX_DEPTH):\r\n",
        "        if depth == 0:\r\n",
        "            return None, 1.0\r\n",
        "        \r\n",
        "        s *= env.turn\r\n",
        "        with torch.no_grad():\r\n",
        "            values = self.q_model(s).reshape(-1)\r\n",
        "        valid_moves = env.get_valid_moves(env.turn)\r\n",
        "        value_moves = []\r\n",
        "        best_value = -1e9\r\n",
        "        best_action = None\r\n",
        "        if len(valid_moves) > 0:\r\n",
        "            for i, move in enumerate(valid_moves):\r\n",
        "                # print(env.render())\r\n",
        "                env_tmp = deepcopy(env)\r\n",
        "                \r\n",
        "                value_moves.append(values[env.coord2ind(move)])\r\n",
        "                new_s, reward, done, info = env_tmp.step(move)\r\n",
        "                new_s = state_numpy_to_tensor(new_s)\r\n",
        "                if done:\r\n",
        "                    if reward * env.turn > 0:\r\n",
        "                        value_moves[-1] *= 1e7\r\n",
        "                    elif reward * env.turn <0:\r\n",
        "                        value_moves[-1] *= -1e7\r\n",
        "                    else:\r\n",
        "                        value_moves[-1] *= 0\r\n",
        "                else:\r\n",
        "                    value_moves[-1] *= self.draw_action_minimax(env_tmp, new_s, depth=depth-1)[1]\r\n",
        "                if best_value < value_moves[-1]:\r\n",
        "                    best_value = value_moves[-1]\r\n",
        "                    best_action = valid_moves[i]\r\n",
        "\r\n",
        "        else:\r\n",
        "            return None, values[env.n * env.n] * (env.turn * self.color)\r\n",
        "\r\n",
        "        return best_action, best_value * (env.turn * self.color)\r\n",
        "\r\n",
        "\r\n",
        "    def update_target_model(self):\r\n",
        "         self.target_model.load_state_dict(self.q_model.state_dict())\r\n",
        "\r\n",
        "class RandomAgent:\r\n",
        "    def __init__(self, color):\r\n",
        "        self.color = color\r\n",
        "    \r\n",
        "    def draw_action(self, env, s, epsilon):\r\n",
        "        valid_moves = env.get_valid_moves(self.color)\r\n",
        "        if len(valid_moves) > 0:\r\n",
        "            action = valid_moves[np.random.randint(0, len(valid_moves))]\r\n",
        "            return action, 1. / len(valid_moves)\r\n",
        "        else:\r\n",
        "            return None, 0\r\n",
        "\r\n",
        "\r\n",
        "class OthelloGame:\r\n",
        "    def __init__(self, agent_white, agent_black):\r\n",
        "        self.white = agent_white\r\n",
        "        self.black = agent_black\r\n",
        "\r\n",
        "    def get_agent(self, color):\r\n",
        "        if color == 1:\r\n",
        "            return self.white\r\n",
        "        else:\r\n",
        "            return self.black\r\n",
        "    \r\n",
        "    def sync(self, color_optimized, color_update):\r\n",
        "        \"\"\"Copy model state of agent color_optimized to agent color_update\"\"\"\r\n",
        "        self.get_agent(color_update).q_model.load_state_dict(self.get_agent(color_optimized).q_model.state_dict())"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipbNdmfFyuqu"
      },
      "source": [
        "Transition = namedtuple('Transition',\r\n",
        "                        ('state', 'action', 'next_state', 'reward'))\r\n",
        "\r\n",
        "class ReplayBuffer(object):\r\n",
        "    def __init__(self, capacity):\r\n",
        "        self.capacity = capacity\r\n",
        "        self.memory = []\r\n",
        "        self.position = 0\r\n",
        "\r\n",
        "    def push(self, *args):\r\n",
        "        \"\"\"Saves a transition.\"\"\"\r\n",
        "        if len(self.memory) < self.capacity:\r\n",
        "            self.memory.append(None)\r\n",
        "        self.memory[self.position] = Transition(*args)\r\n",
        "        self.position = (self.position + 1) % self.capacity\r\n",
        "\r\n",
        "    def sample(self, batch_size):\r\n",
        "        return random.sample(self.memory, batch_size)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.memory)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Duw44fWmIsWm"
      },
      "source": [
        "def optimize_model(agent, batch_size=BATCH_SIZE, device=device, gamma=GAMMA):\r\n",
        "    agent.q_model.train()\r\n",
        "\r\n",
        "    if len(agent.buffer) < batch_size:\r\n",
        "        agent.q_model.eval()\r\n",
        "        return\r\n",
        "\r\n",
        "    transitions = agent.buffer.sample(batch_size)\r\n",
        "    batch = Transition(*zip(*transitions))\r\n",
        "\r\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\r\n",
        "                                            batch.next_state)), device=device, dtype=torch.bool)\r\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\r\n",
        "\r\n",
        "    reward_batch = torch.tensor(batch.reward, device=device)\r\n",
        "    state_batch = torch.cat(batch.state)\r\n",
        "    action_batch = torch.cat(batch.action)\r\n",
        "\r\n",
        "    state_action_values = agent.q_model(state_batch).gather(1, action_batch)\r\n",
        "\r\n",
        "    next_state_values = torch.zeros(batch_size, device=device)\r\n",
        "    next_state_values[non_final_mask] = agent.target_model(non_final_next_states).max(1)[0].detach()\r\n",
        "\r\n",
        "    expected_state_action_values = next_state_values * gamma + reward_batch\r\n",
        "\r\n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\r\n",
        "\r\n",
        "    agent.optimizer.zero_grad()\r\n",
        "    loss.backward()\r\n",
        "    for param in agent.q_model.parameters():\r\n",
        "        param.grad.data.clamp_(-1, 1)\r\n",
        "    agent.optimizer.step()\r\n",
        "    agent.q_model.eval()\r\n",
        "\r\n",
        "def state_numpy_to_tensor(state, device=device):\r\n",
        "    state = torch.from_numpy(state.astype(np.int64)).unsqueeze(0).unsqueeze(0)\r\n",
        "    return state.to(device)\r\n",
        "\r\n",
        "\r\n",
        "def train_one_episode(env, game, color, device=device, batch_size=BATCH_SIZE, gamma=GAMMA, epsilon=None):\r\n",
        "    game.get_agent(color).q_model.eval()\r\n",
        "    game.get_agent(-color).q_model.eval()\r\n",
        "\r\n",
        "    state = env.reset()\r\n",
        "    state = state_numpy_to_tensor(state)\r\n",
        "    done = False\r\n",
        "\r\n",
        "    while not done:\r\n",
        "        # Player plays\r\n",
        "        this_turn = env.turn\r\n",
        "        action, value = game.get_agent(env.turn).draw_action(env, state, epsilon)\r\n",
        "        if action is not None:\r\n",
        "            opp_state, reward, done, info = env.step(action)\r\n",
        "            action = torch.tensor([[env.coord2ind(action)]], device=device, dtype=torch.int64)\r\n",
        "            opp_state = state_numpy_to_tensor(opp_state)\r\n",
        "\r\n",
        "            if reward * color > 0:\r\n",
        "                reward = 1.\r\n",
        "            elif reward * color < 0:\r\n",
        "                reward = -1.\r\n",
        "        else:\r\n",
        "            raise ValueError(\"No valid move!\")\r\n",
        "        \r\n",
        "        if this_turn != color:\r\n",
        "            # If this is not the turn of player color, then skip to next turn\r\n",
        "            continue\r\n",
        "        if this_turn == env.turn or done:\r\n",
        "            # The opponent skipped his turn or the match ended\r\n",
        "            new_state = opp_state\r\n",
        "        else:\r\n",
        "            # Opponent plays\r\n",
        "            while env.turn != color and not done:\r\n",
        "                opp_action, _ = game.get_agent(env.turn).draw_action(env, opp_state, epsilon)\r\n",
        "                if opp_action is not None:\r\n",
        "                    new_state, reward, done, info = env.step(opp_action)\r\n",
        "                    new_state = state_numpy_to_tensor(new_state)\r\n",
        "\r\n",
        "                    if reward * color > 0:\r\n",
        "                        reward = 1.\r\n",
        "                    elif reward * color < 0:\r\n",
        "                        reward = -1.\r\n",
        "                else:\r\n",
        "                    raise ValueError(\"No valid move!\")\r\n",
        "\r\n",
        "        reward = torch.tensor([reward], device=device, dtype=torch.float)\r\n",
        "        # Push the transition into player color's replay buffer\r\n",
        "        game.get_agent(color).buffer.push(state * color, action, new_state * color, reward)\r\n",
        "\r\n",
        "        optimize_model(game.get_agent(color), batch_size, device, gamma)\r\n",
        "\r\n",
        "        # Next turn\r\n",
        "        state = new_state\r\n",
        "\r\n",
        "def score_multi_episode(env, game, color, depth=MINIMAX_DEPTH, device=device, num_episodes=NUM_EPISODES_EVAL, epsilon = .0):\r\n",
        "    '''Trained agent plays against an agent with random policy'''\r\n",
        "    trained_agent = game.get_agent(color)\r\n",
        "    eval_agent = RandomAgent(-color)\r\n",
        "    \r\n",
        "    num_success = 0\r\n",
        "    num_cons_success = [0]\r\n",
        "    results = []\r\n",
        "    score = .0\r\n",
        "\r\n",
        "    for i in range(num_episodes):\r\n",
        "        state = env.reset()\r\n",
        "        state = state_numpy_to_tensor(state)\r\n",
        "        done = False\r\n",
        "        alpha = 0.\r\n",
        "        # alpha_incre = 1.0 / (env.n * env.n - 4)\r\n",
        "        # No use of minimax\r\n",
        "        while not done:\r\n",
        "            # print(env.render())\r\n",
        "            if env.turn == color:\r\n",
        "                if np.random.rand() < alpha:\r\n",
        "                    action, value = trained_agent.draw_action_minimax(env, state, depth)\r\n",
        "                else:\r\n",
        "                    action, value = trained_agent.draw_action(env, state, epsilon)\r\n",
        "            else:\r\n",
        "                action, value = eval_agent.draw_action(env, state, epsilon)\r\n",
        "            if action is not None:\r\n",
        "                state, reward, done, info = env.step(action)\r\n",
        "                state = state_numpy_to_tensor(state)\r\n",
        "            else:\r\n",
        "                done = env.score() is not None and env.turn_passed\r\n",
        "                env.turn *= -1\r\n",
        "\r\n",
        "            # alpha += alpha_incre\r\n",
        "        \r\n",
        "        # print(env.render())\r\n",
        "        if reward * color > 0:\r\n",
        "            num_success += 1\r\n",
        "            num_cons_success[-1] += 1\r\n",
        "            score += 1.\r\n",
        "        else:\r\n",
        "            num_cons_success.append(0)\r\n",
        "            if env.score() != 0:\r\n",
        "                score -= 1.\r\n",
        "\r\n",
        "        results.append(reward)\r\n",
        "    return num_success, max(num_cons_success), score, results"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExtJv7uBVHgO"
      },
      "source": [
        "np.random.seed(1)\r\n",
        "torch.manual_seed(0)\r\n",
        "random.seed(0)\r\n",
        "\r\n",
        "game = OthelloGame(DQNAgent(env, 1, lr=LR), DQNAgent(env, -1, lr=LR))\r\n",
        "game.sync(1, -1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgGNgAipmgwG"
      },
      "source": [
        "color = 1\r\n",
        "nb_episodes_per_agent = 1\r\n",
        "target_update = 10\r\n",
        "print_step = 500\r\n",
        "\r\n",
        "for i in tqdm(range(200001)):\r\n",
        "    train_one_episode(env, game, color)\r\n",
        "\r\n",
        "    # if i % 100 == 0:\r\n",
        "    #     print(env.render())\r\n",
        "    #     print(env.score())\r\n",
        "\r\n",
        "    if i % nb_episodes_per_agent == 0:\r\n",
        "        game.sync(color, -color)  # Update model for the other player\r\n",
        "        color *= -1\r\n",
        "    \r\n",
        "    if i % target_update == 0:\r\n",
        "        game.get_agent(color).update_target_model()\r\n",
        "        game.get_agent(-color).update_target_model()\r\n",
        "\r\n",
        "    if i % print_step == 0:\r\n",
        "        print(\"Scoring...\")\r\n",
        "        num_success, max_cons_success, score, _ = score_multi_episode(env, game, 1)\r\n",
        "        print(\"White ... Episode: {}, Number of wins: {}, Max number of consecutive wins: {}, Total score: {:.1f}\".format(i, num_success, max_cons_success, score))\r\n",
        "        num_success, max_cons_success, score, _ = score_multi_episode(env, game, -1)\r\n",
        "        print(\"Black ... Episode: {}, Number of wins: {}, Max number of consecutive wins: {}, Total score: {:.1f}\".format(i, num_success, max_cons_success, score))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}